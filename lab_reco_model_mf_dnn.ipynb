{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, json, time\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, itertools, shutil\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict, Counter\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "randomSeed = 88\n",
    "np.random.seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Data Prepare\n",
    "- mapping user id, movie id to zero start indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100004, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>833</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>859</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>906</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>931</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       0       30     2.5  1260759144\n",
       "1       0      833     3.0  1260759179\n",
       "2       0      859     3.0  1260759182\n",
       "3       0      906     2.0  1260759185\n",
       "4       0      931     4.0  1260759205"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"./data/ml-latest-small/ratings.csv\")\n",
    "movies = pd.read_csv(\"./data/ml-latest-small/movies.csv\")\n",
    "tags = pd.read_csv(\"./data/ml-latest-small/tags.csv\")\n",
    "\n",
    "uidEnc, midEnc = LabelEncoder(), LabelEncoder()\n",
    "# encode user id and movie id to real value\n",
    "midEnc.fit(movies.movieId)\n",
    "uidEnc.fit(ratings.userId)\n",
    "\n",
    "ratings[\"userId\"] = uidEnc.transform(ratings.userId)\n",
    "ratings[\"movieId\"] = midEnc.transform(ratings.movieId)\n",
    "\n",
    "movies[\"movieId\"] = midEnc.transform(movies.movieId)\n",
    "\n",
    "tags[\"userId\"] = uidEnc.transform(tags.userId)\n",
    "tags[\"movieId\"] = midEnc.transform(tags.movieId)\n",
    "\n",
    "midMap = pd.Series(dict(zip(movies.movieId, movies.title)))\n",
    "\n",
    "nUsers, nMovies = len(uidEnc.classes_), len(midEnc.classes_)\n",
    "\n",
    "print(ratings.shape)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        0                    Toy Story (1995)   \n",
       "1        1                      Jumanji (1995)   \n",
       "2        2             Grumpier Old Men (1995)   \n",
       "3        3            Waiting to Exhale (1995)   \n",
       "4        4  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movies profile\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>tag</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>304</td>\n",
       "      <td>sandra 'boring' bullock</td>\n",
       "      <td>1138537770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1517</td>\n",
       "      <td>dentist</td>\n",
       "      <td>1193435061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>5166</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>1170560997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>6118</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1170626366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>6178</td>\n",
       "      <td>forgettable</td>\n",
       "      <td>1141391765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>6208</td>\n",
       "      <td>short</td>\n",
       "      <td>1141391873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>6236</td>\n",
       "      <td>dull story</td>\n",
       "      <td>1141391806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>6453</td>\n",
       "      <td>powerpoint</td>\n",
       "      <td>1169616291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>8268</td>\n",
       "      <td>activist</td>\n",
       "      <td>1425876220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>8268</td>\n",
       "      <td>documentary</td>\n",
       "      <td>1425876220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId                      tag   timestamp\n",
       "0      14      304  sandra 'boring' bullock  1138537770\n",
       "1      14     1517                  dentist  1193435061\n",
       "2      14     5166                 Cambodia  1170560997\n",
       "3      14     6118                  Russian  1170626366\n",
       "4      14     6178              forgettable  1141391765\n",
       "5      14     6208                    short  1141391873\n",
       "6      14     6236               dull story  1141391806\n",
       "7      14     6453               powerpoint  1169616291\n",
       "8      14     8268                 activist  1425876220\n",
       "9      14     8268              documentary  1425876220"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags profile\n",
    "tags.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "### Split Train and Test Data\n",
    "1. 拆分train test data\n",
    "2. number of user = 671, number of movies = 9125 => movie數量越大於user數量則越貼近現實\n",
    "3. 組合interaction matrix of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 已經產生出檔案的不用跑這一段\n",
    "# tr, te = utils.split_ratings(ratings, testRatio=0.3)\n",
    "# tr.to_csv(\"./data/ml-latest-small/movielens.tr.csv\", index=False)\n",
    "# te.to_csv(\"./data/ml-latest-small/movielens.te.csv\", index=False)\n",
    "# utils.dumpPickle(\"./data/ml-latest-small/state.h\", \n",
    "#         {\"uidEnc\": uidEnc, \"midEnc\": midEnc, \"midMap\": midMap, \"nUsers\": nUsers, \"nMovies\":nMovies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train interaction matrix shape:  (671, 9125) test interaction matrix shape:  (671, 9125)\n",
      "train.shape:  (69399, 4) test.shape:  (30605, 4)\n",
      "\n",
      "   userId  movieId  rating   timestamp\n",
      "0       0      931     4.0  1260759205\n",
      "1       0     1515     4.0  1260759191\n",
      "2       0       30     2.5  1260759144\n",
      "3       0      833     3.0  1260759179\n",
      "4       0      859     3.0  1260759182\n",
      "\n",
      "   userId  movieId  rating   timestamp\n",
      "0       0     1665     4.0  1260759139\n",
      "1       0     1708     3.0  1260759194\n",
      "2       0     1743     2.0  1260759198\n",
      "3       0     1815     2.0  1260759108\n",
      "4       0     1962     2.5  1260759113\n"
     ]
    }
   ],
   "source": [
    "tr = pd.read_csv(\"./data/ml-latest-small/movielens.tr.csv\")\n",
    "te = pd.read_csv(\"./data/ml-latest-small/movielens.te.csv\")\n",
    "\n",
    "# state = utils.loadPickle(\"./data/ml-latest-small/state.h\")\n",
    "# uidEnc, midEnc, midMap, nUsers, nMovies = \\\n",
    "#     (state[\"uidEnc\"], state[\"midEnc\"], state[\"midMap\"], state[\"nUsers\"], state[\"nMovies\"])\n",
    "\n",
    "# train data rating matrix\n",
    "trRatingMat = np.zeros((nUsers, nMovies))\n",
    "# test data rating matrix\n",
    "teRatingMat = np.zeros((nUsers, nMovies))\n",
    "for idx, r in tr.iterrows():\n",
    "    trRatingMat[int(r.userId), int(r.movieId)] = r.rating\n",
    "for idx, r in te.iterrows():\n",
    "    teRatingMat[int(r.userId), int(r.movieId)] = r.rating\n",
    "\n",
    "print(\"train interaction matrix shape: \", trRatingMat.shape, \"test interaction matrix shape: \", teRatingMat.shape)\n",
    "print(\"train.shape: \", tr.shape, \"test.shape: \", te.shape)\n",
    "print()\n",
    "print(tr.head())\n",
    "print()\n",
    "print(te.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以leave one out方式產生 train data, test data\n",
    "1. 每一筆資料有兩部分: [user query] + [item meta]\n",
    "2. movie多加了兩個欄位: avg_rating, year\n",
    "    - avg_rating是所有user對於同一部電影的平均評分\n",
    "    - year則是從title extract出來的資訊\n",
    "3. 每一筆user query 包含所有user movie history, 除了當前的rating movie (candidate movie)\n",
    "4. test data的user query來自於train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1515, 1083, 833, 859, 30, 1111, 906, 1017, 10...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.835749</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>931</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1083, 833, 859, 30, 1111, 906, 1017, 104...</td>\n",
       "      <td>[4, 6, 2]</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1515</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1515, 833, 859, 30, 1111, 906, 1017, 104...</td>\n",
       "      <td>[9, 7, 3, 2]</td>\n",
       "      <td>0.621795</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>1083</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1515, 1083, 859, 30, 1111, 906, 1017, 10...</td>\n",
       "      <td>[13, 10, 0, 14]</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>833</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1515, 1083, 833, 30, 1111, 906, 1017, 10...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>859</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        0  [1515, 1083, 833, 859, 30, 1111, 906, 1017, 10...   \n",
       "1        0  [931, 1083, 833, 859, 30, 1111, 906, 1017, 104...   \n",
       "2        0  [931, 1515, 833, 859, 30, 1111, 906, 1017, 104...   \n",
       "3        0  [931, 1515, 1083, 859, 30, 1111, 906, 1017, 10...   \n",
       "4        0  [931, 1515, 1083, 833, 30, 1111, 906, 1017, 10...   \n",
       "\n",
       "            genres  avg_rating      year  candidate_movie_id  rating  \n",
       "0              [0]    0.835749  0.763158                 931     4.0  \n",
       "1        [4, 6, 2]    0.782609  0.605263                1515     4.0  \n",
       "2     [9, 7, 3, 2]    0.621795  0.789474                1083     3.5  \n",
       "3  [13, 10, 0, 14]    0.711640  0.342105                 833     3.0  \n",
       "4              [2]    0.676768  0.824561                 859     3.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data, movie_trans, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    data = data.merge(movie_trans, how=\"left\", on=\"movieId\")\n",
    "    columns=[\"user_id\", \"query_movie_ids\", \n",
    "             \"genres\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "             \"rating\"]\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                queue.append([int(r.userId), df.movieId[:i].tolist() + df.movieId[i + 1:].tolist(), r.genres, r.avg_rating, r.year, int(r.movieId), r.rating])\n",
    "            else:\n",
    "                # queue.append([int(r.userId), df.movieId[:i].tolist() + df.movieId[i + 1:].tolist(), r.genres, r.avg_rating, r.year, int(r.movieId), r.rating])\n",
    "                all_hist = set(user_movies_hist.tolist() + df.movieId[:i].tolist())\n",
    "                queue.append([int(r.userId), list(all_hist - set([int(r.movieId)])), r.genres, r.avg_rating, r.year, int(r.movieId), r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "\n",
    "movie_trans, genres_enc = utils.doMovies(movies)\n",
    "movie_trans[\"avg_rating\"] = ratings.groupby(\"movieId\").rating.mean()\n",
    "movie_trans[\"avg_rating\"] = minmax_scale(movie_trans.avg_rating.fillna(ratings.rating.mean()))\n",
    "movie_trans[\"year\"] = movie_trans.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "movie_trans[\"year\"] = minmax_scale(movie_trans.year.fillna(movie_trans.year.median()))\n",
    "n_genres = len(genres_enc.enc)\n",
    "\n",
    "trProcessed = preprocess(tr, movie_trans)\n",
    "teProcessed = preprocess(te, movie_trans, tr, is_train=False)\n",
    "trProcessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[833, 931, 1083, 906, 1515, 1041, 1140, 1111, ...</td>\n",
       "      <td>[4, 5, 8]</td>\n",
       "      <td>0.661939</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>1665</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[833, 1665, 931, 1083, 906, 1515, 1041, 1140, ...</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1708</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[833, 1665, 931, 1083, 906, 1515, 1708, 1041, ...</td>\n",
       "      <td>[1, 16]</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>2925</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[833, 1665, 906, 1041, 1111, 1047, 859, 30, 93...</td>\n",
       "      <td>[0, 7, 8, 2]</td>\n",
       "      <td>0.643026</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>1962</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[833, 1665, 906, 1041, 1111, 1047, 859, 30, 93...</td>\n",
       "      <td>[4, 5, 9]</td>\n",
       "      <td>0.600529</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>1743</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids        genres  \\\n",
       "0        0  [833, 931, 1083, 906, 1515, 1041, 1140, 1111, ...     [4, 5, 8]   \n",
       "1        0  [833, 1665, 931, 1083, 906, 1515, 1041, 1140, ...        [5, 1]   \n",
       "2        0  [833, 1665, 931, 1083, 906, 1515, 1708, 1041, ...       [1, 16]   \n",
       "3        0  [833, 1665, 906, 1041, 1111, 1047, 859, 30, 93...  [0, 7, 8, 2]   \n",
       "4        0  [833, 1665, 906, 1041, 1111, 1047, 859, 30, 93...     [4, 5, 9]   \n",
       "\n",
       "   avg_rating      year  candidate_movie_id  rating  \n",
       "0    0.661939  0.701754                1665     4.0  \n",
       "1    0.669753  0.684211                1708     3.0  \n",
       "2    0.763441  0.631579                2925     3.0  \n",
       "3    0.643026  0.736842                1962     2.5  \n",
       "4    0.600529  0.754386                1743     2.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teProcessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Function\n",
    "1. 由於tensorflow placeholder不支援變動長度的columns, 需透過padding zero(補零)帶入\n",
    "2. 每個變動長度的column, 需要再給lens描述每一筆資料的長度, ex: genres, genres_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>query_movie_ids_len</th>\n",
       "      <th>genres</th>\n",
       "      <th>genres_len</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>[263, 2860, 3856, 6034, 2409, 2374, 7128, 5339...</td>\n",
       "      <td>357</td>\n",
       "      <td>[1, 0, 3, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1393</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>[966, 5026, 4395, 6042, 3871, 6521, 5020, 953,...</td>\n",
       "      <td>177</td>\n",
       "      <td>[5, 13, 10, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.643739</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>4002</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261</td>\n",
       "      <td>[45, 1104, 154, 4514, 4171, 3801, 3727, 3644, ...</td>\n",
       "      <td>471</td>\n",
       "      <td>[0, 7, 2, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.545752</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>3089</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>[0, 4815, 1, 4557, 4398, 4395, 969, 4325, 1019...</td>\n",
       "      <td>263</td>\n",
       "      <td>[7, 11, 2, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.448148</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1308</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345</td>\n",
       "      <td>[3595, 3506, 2308, 3319, 3299, 1379, 3290, 275...</td>\n",
       "      <td>229</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.779449</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>2340</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0       47  [263, 2860, 3856, 6034, 2409, 2374, 7128, 5339...   \n",
       "1      175  [966, 5026, 4395, 6042, 3871, 6521, 5020, 953,...   \n",
       "2      261  [45, 1104, 154, 4514, 4171, 3801, 3727, 3644, ...   \n",
       "3      479  [0, 4815, 1, 4557, 4398, 4395, 969, 4325, 1019...   \n",
       "4      345  [3595, 3506, 2308, 3319, 3299, 1379, 3290, 275...   \n",
       "\n",
       "   query_movie_ids_len          genres  genres_len  avg_rating      year  \\\n",
       "0                  357    [1, 0, 3, 0]           3    0.716049  0.833333   \n",
       "1                  177  [5, 13, 10, 1]           4    0.643739  0.877193   \n",
       "2                  471    [0, 7, 2, 0]           3    0.545752  0.859649   \n",
       "3                  263   [7, 11, 2, 0]           3    0.448148  0.833333   \n",
       "4                  229    [1, 0, 0, 0]           1    0.779449  0.736842   \n",
       "\n",
       "   candidate_movie_id  rating  \n",
       "0                1393     4.5  \n",
       "1                4002     3.0  \n",
       "2                3089     2.0  \n",
       "3                1308     3.0  \n",
       "4                2340     1.5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_multi(df, multi_cols):\n",
    "    \"\"\"對於multivalent的欄位, 需要增加一個column去描述該欄位的長度\"\"\"\n",
    "    pad = tf.keras.preprocessing.sequence.pad_sequences\n",
    "    ret = OrderedDict()\n",
    "    for colname, col in df.iteritems():\n",
    "        if colname in multi_cols:\n",
    "            lens = col.map(len)\n",
    "            ret[colname] = list(pad(col, padding=\"post\", maxlen=lens.max()))\n",
    "            ret[colname + \"_len\"] = lens.values\n",
    "        else:\n",
    "            ret[colname] = col.values\n",
    "    return ret\n",
    "\n",
    "def dataFn(data, n_batch=128, shuffle=False):\n",
    "    pad = tf.keras.preprocessing.sequence.pad_sequences\n",
    "    def fn():\n",
    "        dataInner = data.copy()\n",
    "        indices = utils.get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n",
    "        for ind in indices:\n",
    "            yield do_multi(dataInner.iloc[ind], [\"query_movie_ids\", \"genres\"])\n",
    "    return fn\n",
    "\n",
    "for i, e in enumerate(dataFn(trProcessed, n_batch=5, shuffle=True)(), 1):\n",
    "    # print(e)\n",
    "    break\n",
    "pd.DataFrame(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF + DNN with Pointwise Loss Function (RMSE or L2 loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelMfDNN(object):\n",
    "    def __init__(self,\n",
    "                 n_items,\n",
    "                 n_genres,\n",
    "                 dim=32,\n",
    "                 learning_rate=0.01,\n",
    "                 modelDir=\"./model/model_mf_with_dnn\"):\n",
    "        \"\"\"初始化 Tensorflow Graph\"\"\"\n",
    "        self.n_items = n_items\n",
    "        self.n_genres = n_genres\n",
    "        self.ftr_cols = OrderedDict()\n",
    "\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            # inputs/id_user:0\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.isTrain = tf.placeholder(tf.bool, None)\n",
    "                # user data\n",
    "                self.user_id = tf.placeholder(tf.int32, [None])\n",
    "                self.query_movie_ids = tf.placeholder(tf.int32, [None, None])\n",
    "                self.query_movie_ids_len = tf.placeholder(tf.int32, [None])\n",
    "                # item data\n",
    "                self.genres = tf.placeholder(tf.int32, [None, None])\n",
    "                self.genres_len = tf.placeholder(tf.int32, [None])\n",
    "                self.avg_rating = tf.placeholder(tf.float32, [None])\n",
    "                self.year = tf.placeholder(tf.float32, [None])\n",
    "                self.candidate_movie_id = tf.placeholder(tf.int32, [None])\n",
    "                self.rating = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            emb_init_fn = tf.glorot_uniform_initializer()\n",
    "            self.b_global = tf.Variable(emb_init_fn(shape=[]), name=\"b_global\")\n",
    "            with tf.variable_scope(\"embedding\"):\n",
    "                self.w_query_movie_ids = tf.Variable(emb_init_fn(shape=[self.n_items, dim]), name=\"w_query_movie_ids\")\n",
    "                self.b_query_movie_ids = tf.Variable(emb_init_fn(shape=[dim]), name=\"b_query_movie_ids\")\n",
    "                self.w_candidate_movie_id = tf.Variable(init_fn(shape=[self.n_items, dim]), name=\"w_candidate_movie_id\")\n",
    "                self.b_candidate_movie_id = tf.Variable(init_fn(shape=[dim + 8 + 2]), name=\"b_candidate_movie_id\")\n",
    "                self.w_genres = tf.Variable(emb_init_fn(shape=[self.n_genres, 8]), name=\"w_genres\")\n",
    "\n",
    "                # query_movie embedding\n",
    "                self.query_emb = tf.nn.embedding_lookup(self.w_query_movie_ids, self.query_movie_ids)\n",
    "                query_movie_mask = tf.expand_dims(tf.nn.l2_normalize(tf.to_float(tf.sequence_mask(self.query_movie_ids_len)), 1), -1)\n",
    "                self.query_emb = tf.reduce_sum(self.query_emb * query_movie_mask, 1)\n",
    "                self.query_bias = tf.matmul(self.query_emb, self.b_query_movie_ids[:, tf.newaxis])\n",
    "\n",
    "                # candidate_movie embedding\n",
    "                self.candidate_emb = tf.nn.embedding_lookup(self.w_candidate_movie_id, self.candidate_movie_id)\n",
    "\n",
    "                # genres embedding\n",
    "                self.genres_emb = tf.nn.embedding_lookup(self.w_genres, tf.to_int32(self.genres))\n",
    "                genres_mask = tf.expand_dims( tf.nn.l2_normalize(tf.to_float(tf.sequence_mask( tf.reshape(self.genres_len, [-1])) ), 1), -1)\n",
    "                self.genres_emb = tf.reduce_sum(self.genres_emb * genres_mask, 1)\n",
    "            \n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                # encode [item embedding + item metadata]\n",
    "                self.item_repr = tf.concat([self.candidate_emb, self.genres_emb, self.avg_rating[:, tf.newaxis], self.year[:, tf.newaxis]], 1)\n",
    "                self.candidate_bias = tf.matmul(self.item_repr, self.b_candidate_movie_id[:, tf.newaxis])\n",
    "                \n",
    "                # Do: 若有overfitting後可嘗試dropout\n",
    "                # dp_scale = 0.5\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "                \n",
    "                \n",
    "            with tf.variable_scope(\"computation\"):\n",
    "                infer = tf.reduce_sum(self.query_emb * self.item_repr, 1, keep_dims=True)\n",
    "                infer = tf.add(infer, self.b_global)\n",
    "                infer = tf.add(infer, self.query_bias)\n",
    "                self.infer = tf.add(infer, self.candidate_bias, name=\"infer\")\n",
    "\n",
    "                # one query for all items\n",
    "                self.pred = tf.matmul(self.query_emb, tf.transpose(self.item_repr)) + \\\n",
    "                            tf.reshape(self.candidate_bias, (1, -1)) + self.query_bias + self.b_global\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.losses.mean_squared_error(labels=self.rating[:, tf.newaxis], predictions=self.infer)\n",
    "\n",
    "                # for eval\n",
    "                self.rmse_loss = tf.sqrt(self.loss)\n",
    "                self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.rating[:, tf.newaxis]))\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                # Do: 嘗試不同的Optimizer\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "                pass\n",
    "\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            self.graph = graph\n",
    "            self.modelDir = modelDir\n",
    "\n",
    "    def resetModel(self, modelDir):\n",
    "        shutil.rmtree(path=modelDir, ignore_errors=True)\n",
    "        os.makedirs(modelDir)\n",
    "\n",
    "    def feed_dict(self, data, mode=\"train\"):\n",
    "        ret = {\n",
    "            self.user_id: data[\"user_id\"],\n",
    "            self.query_movie_ids: data[\"query_movie_ids\"],\n",
    "            self.query_movie_ids_len: data[\"query_movie_ids_len\"],\n",
    "            self.genres: data[\"genres\"],\n",
    "            self.genres_len: data[\"genres_len\"],\n",
    "            self.avg_rating: data[\"avg_rating\"],\n",
    "            self.year: data[\"year\"],\n",
    "            self.candidate_movie_id: data[\"candidate_movie_id\"]\n",
    "        }\n",
    "        ret[self.isTrain] = False\n",
    "        if mode != \"infer\":\n",
    "            ret[self.rating] = data[\"rating\"]\n",
    "            if mode == \"train\":\n",
    "                ret[self.isTrain] = True\n",
    "            elif mode == \"eval\":\n",
    "                pass\n",
    "        return ret\n",
    "\n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if reset:\n",
    "            print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "            self.resetModel(self.modelDir)\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "        minLoss = 1e7\n",
    "        for ep in range(1, nEpoch + 1):\n",
    "            tr_loss, tr_total = 0, 0\n",
    "            for i, data in enumerate(trainGen(), 1):\n",
    "                loss, _ = sess.run([self.rmse_loss, self.train_op], feed_dict=self.feed_dict(data, mode=\"train\"))\n",
    "                tr_loss += loss ** 2 * len(data[\"query_movie_ids\"])\n",
    "                tr_total += len(data[\"query_movie_ids\"])\n",
    "                print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "            if testGen is not None:\n",
    "                epochLoss = self.epochLoss(sess, testGen)\n",
    "\n",
    "            tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "            if minLoss > epochLoss:\n",
    "                tpl += \", saving ...\"\n",
    "                self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "                minLoss = epochLoss\n",
    "\n",
    "            end = time.time()\n",
    "            print(tpl % (ep, np.sqrt(tr_loss / tr_total), epochLoss, end - start))\n",
    "            start = end\n",
    "        return self\n",
    "\n",
    "    def ckpt(self, sess, modelDir):\n",
    "        \"\"\"load latest saved model\"\"\"\n",
    "        latestCkpt = tf.train.latest_checkpoint(modelDir)\n",
    "        if latestCkpt:\n",
    "            self.saver.restore(sess, latestCkpt)\n",
    "        return latestCkpt\n",
    "\n",
    "    def epochLoss(self, sess, dataGen, tpe=\"rmse\"):\n",
    "        totLoss, totCnt = 0, 0\n",
    "        for data in dataGen():\n",
    "            lossTensor = self.rmse_loss if tpe == \"rmse\" else self.mae_loss\n",
    "            loss = sess.run(lossTensor, feed_dict=self.feed_dict(data, mode=\"eval\"))\n",
    "            totLoss += (loss ** 2 if tpe == \"rmse\" else loss) * len(data[\"query_movie_ids\"])\n",
    "            totCnt += len(data[\"query_movie_ids\"])\n",
    "        return np.sqrt(totLoss / totCnt) if tpe == \"rmse\" else totLoss / totCnt\n",
    "\n",
    "    def predict(self, sess, user_queries, items):\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return sess.run(self.pred, feed_dict={\n",
    "            self.isTrain: False,\n",
    "            self.user_id: user_queries[\"user_id\"],\n",
    "            self.query_movie_ids: user_queries[\"query_movie_ids\"],\n",
    "            self.query_movie_ids_len: user_queries[\"query_movie_ids_len\"],\n",
    "\n",
    "            self.genres: items[\"genres\"],\n",
    "            self.genres_len: items[\"genres_len\"],\n",
    "            self.avg_rating: items[\"avg_rating\"],\n",
    "            self.year: items[\"year\"],\n",
    "            self.candidate_movie_id: items[\"candidate_movie_id\"]\n",
    "        })\n",
    "\n",
    "    def evaluateRMSE(self, sess, dataGen):\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"rmse\")\n",
    "\n",
    "    def evaluateMAE(self, sess, dataGen):\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"mae\")\n",
    "\n",
    "# hyper parameters\n",
    "# Do: 嘗試不同的learning_rate [0.1, 0.001, 0.0001]\n",
    "learning_rate = 0.0001\n",
    "# Do: 嘗試不同的dim [8, 16, 20, 32]\n",
    "dim = 32\n",
    "# Do: 嘗試不同的reg係數 [0.01, 0.005, 0.0001]\n",
    "modelDir = \"./model/model_mf_with_dnn\"\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "model = ModelMfDNN(\n",
    "            n_items=nMovies,\n",
    "            n_genres=n_genres,\n",
    "            dim=dim,\n",
    "            learning_rate=learning_rate,\n",
    "            modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !cp ./model/model_mf_with_dnn_bak/* ./model/model_mf_with_dnn/\n",
    "# 每個batch的數量\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    # Do: 參數reset = False可以接續train上次儲存的model, 預設True是清空model dir重來\n",
    "    model.fit(sess, dataFn(trProcessed, n_batch=n_batch, shuffle=True), dataFn(teProcessed, n_batch=n_batch), nEpoch=10, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Data Function For Predict\n",
    "1. For fast prediction, make another function\n",
    "2. 原始的dataFn是 user meta + item meta, 若預測671 users對上9125 items的評分需要671 x 9125筆資料, serving time會太久\n",
    "3. 現在預測所有users對所有items的資料只需要671 + 9125筆資料, 執行graph中的pred節點即可\n",
    "   (請在ModelMfDNN的 __init__ method 找 self.pred operation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>query_movie_ids_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1515, 1083, 833, 859, 30, 1111, 906, 1017, 10...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[16, 129, 195, 237, 238, 9, 280, 454, 447, 427...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[321, 954, 266, 1359, 100, 1590, 2288, 1834, 2...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1032, 1019, 1012, 1011, 997, 994, 985, 981, 9...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[1120, 839, 529, 2156, 447, 1393, 1199, 3113, ...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        0  [1515, 1083, 833, 859, 30, 1111, 906, 1017, 10...   \n",
       "1        1  [16, 129, 195, 237, 238, 9, 280, 454, 447, 427...   \n",
       "2        2  [321, 954, 266, 1359, 100, 1590, 2288, 1834, 2...   \n",
       "3        3  [1032, 1019, 1012, 1011, 997, 994, 985, 981, 9...   \n",
       "4        4  [1120, 839, 529, 2156, 447, 1393, 1199, 3113, ...   \n",
       "\n",
       "   query_movie_ids_len  \n",
       "0                   13  \n",
       "1                   52  \n",
       "2                   35  \n",
       "3                  142  \n",
       "4                   69  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns = [\"user_id\", \"query_movie_ids\", \"query_movie_ids_len\",\n",
    "#            \"genres\", \"genres_len\", \"avg_rating\", \"year\", \"candidate_movie_id\",\n",
    "#            \"rating\"]\n",
    "def user_item_data(data, uids, movie_trans, n_batch=128):\n",
    "    u_col = [\"user_id\", \"query_movie_ids\"]\n",
    "    cache = {\"u_ary\": []}\n",
    "    items = do_multi(movie_trans, [\"genres\"])\n",
    "    items[\"candidate_movie_id\"] = items.pop(\"movieId\")\n",
    "    def clear(u_ary):\n",
    "        u_data = do_multi(pd.DataFrame(data=u_ary, columns=u_col), [\"query_movie_ids\"])\n",
    "        cache[\"u_ary\"] = []\n",
    "        return u_data\n",
    "    \n",
    "    for uid, df in data[data.user_id.isin(uids)].groupby(\"user_id\"):\n",
    "        u_rec, u_ary = df.iloc[0], cache[\"u_ary\"]\n",
    "        # print(u_rec.query_movie_ids, u_rec.candidate_movie_id)\n",
    "        u_rec.set_value(\"query_movie_ids\", u_rec.query_movie_ids + [u_rec.candidate_movie_id])\n",
    "        u_ary.append(u_rec[u_col].values)\n",
    "        if len(u_ary) >= n_batch:\n",
    "            yield clear(u_ary), items\n",
    "    yield clear(u_ary), items\n",
    "    \n",
    "for u_data, items in user_item_data(trProcessed, np.arange(0, 5), movie_trans, n_batch=5):\n",
    "    break\n",
    "pd.DataFrame(u_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>genres_len</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>[5, 13, 10, 1, 9, 0, 0, 0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.749438</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>[5, 10, 9, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>[1, 3, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.591337</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>[1, 0, 3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.418803</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title                            genres  \\\n",
       "0                    Toy Story (1995)  [5, 13, 10, 1, 9, 0, 0, 0, 0, 0]   \n",
       "1                      Jumanji (1995)   [5, 10, 9, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2             Grumpier Old Men (1995)    [1, 3, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3            Waiting to Exhale (1995)    [1, 0, 3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  Father of the Bride Part II (1995)    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "   genres_len  avg_rating      year  candidate_movie_id  \n",
       "0           5    0.749438  0.815789                   0  \n",
       "1           3    0.644860  0.815789                   1  \n",
       "2           2    0.591337  0.815789                   2  \n",
       "3           3    0.418803  0.815789                   3  \n",
       "4           1    0.615079  0.815789                   4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(items).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 單一user rating分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_mf_with_dnn\\model-9\n",
      "shape:  (1, 9125) [[ 0.7061  0.6029  0.5552 ...,  0.9257  0.6671  0.643 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAFACAYAAACY6/lAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20fdf5d1a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred = model.predict(sess, u_queries, movies_meta)\n",
    "print(\"shape: \", pred.shape, minmax_scale(pred.T).T)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribution\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribution\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    mae_ = model.evaluateMAE(sess, dataFn(teProcessed, n_batch=n_batch, shuffle=False))\n",
    "    rmse_ = model.evaluateRMSE(sess, dataFn(teProcessed, n_batch=n_batch, shuffle=False))\n",
    "\n",
    "print()\n",
    "print(\"MAE loss: \", mae_)\n",
    "print(\"RMSE loss: \", rmse_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 26\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "    recomm = model.predict(sess, u_queries, movies_meta).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)), \n",
    "              \"title\": midMap[np.arange(len(recomm))].values, \n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# .query(\"rating != 0\")\n",
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_CURVE (Receiver operating characteristic), AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def drawRocCurve(y, predProba):\n",
    "    fprRf, tprRf, _ = roc_curve(y, predProba, pos_label=1)\n",
    "    aucScr = auc(fprRf, tprRf)\n",
    "    print(\"auc:\", aucScr)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fprRf, tprRf, label='ROC CURVE')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('AOC: Area Under Curve (score: {:.4f})'.format(aucScr))\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    predMat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        predMat.append(model.predict(sess, u_data, items))\n",
    "        # predMat.append(model.predict(sess, u_data))\n",
    "    predMat = np.vstack(predMat)\n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pos_ary, neg_ary = [], []\n",
    "for label in teRatingMat:\n",
    "    label = label[label != 0]\n",
    "    pos_ary.append(sum(label >= 4))\n",
    "    neg_ary.append(sum(label < 4))\n",
    "    # print(\"pos: {}, neg: {}\".format(sum(label >= 4), sum(label < 4)))\n",
    "    \n",
    "def draw_pos_neg(idx):\n",
    "    pd.DataFrame(\n",
    "        index=idx,\n",
    "        data={\"pos\": np.array(pos_ary)[idx], \"neg\": np.array(neg_ary)[idx]}).plot.bar(figsize=(10, 5), alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "# 可調整index觀察各user rating分布 ex: [0:10] [10:20] [600:610]\n",
    "draw_pos_neg(np.arange(len(teRatingMat))[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Test Data Movie Ratings(觀察上圖)\n",
    "1. 0號, 2號, 5號, 9號 user 正向評價數量 < 10, 就算model全部預測命中, 命中率也不會是 100%!\n",
    "    ex: 0號user只有1個正向評價, 全部命中也指得到0.1的分數\n",
    "2. 3號user正向評價是負向評價的5倍多, 就算亂猜, 中的機率也很高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = sum(np.sum(teRatingMat >= 4, 1) < 10)\n",
    "print(\"{} 個user正向評價總數小於10!\".format(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strict_condition(label):\n",
    "    label = label[label != 0]\n",
    "    pos, neg = sum(label >= 4), sum(label < 4)\n",
    "    return len(label) >= 10 and pos <= neg and pos > 0\n",
    "    \n",
    "print(\"rating數量 >= 10 且 負評價數量 >= 正評價數量 有 [{}] 人\".format(sum(strict_condition(label) for label in teRatingMat)))\n",
    "\n",
    "def norm_condition(label):\n",
    "    label = label[label != 0]\n",
    "    return sum(label >= 4) > 0 and sum(label < 4) > 0\n",
    "\n",
    "print(\"rating正評價數量 >= 0 且 rating負評價數量 >= 0 有 [{}] 人\".format(sum(norm_condition(label) for label in teRatingMat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision At K: \n",
    "> **預測分數高(rating >= 4)的前10部電影, 和實際user rating比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def precision_at_k(truth, pred_mat, condition_fn=None, k=10, label_thres=4):\n",
    "    hits, total = 0, 0\n",
    "    for label, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(label): continue\n",
    "\n",
    "        top_k_ind = (pr * (label != 0)).argsort()[::-1][:k]\n",
    "        hits += sum(label[top_k_ind] >= label_thres)\n",
    "        total += k\n",
    "    return hits / total\n",
    "\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG: Normalized Discounted Cumulative Gain\n",
    "1. A measure of ranking quality.\n",
    "2. loop 每一位user, prediciton score排序後計算NDCG\n",
    "    <br/>$$ DCG_p = \\sum^p_{i = 1} \\frac{2^{rel_i} - 1}{log_2(i + 1)} $$<br/>\n",
    "3. IDCG: Ideal DCG, 為理想狀態下的DCG分數, 即model全部命中的DCG分數, 而NDCG: Normalized DCG, 公式如下\n",
    "    <br/>$$ NDCG_p = \\sum^p_{i = 1} \\frac{DCG_p}{IDCG_p} $$<br/>\n",
    "4. 所以NDCG是一個比值, 介於0 ~ 1之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def single_user_ndcg(label, score, label_thres=4, k=10):\n",
    "    \"\"\"single user ndcg score\"\"\"\n",
    "    nnz = label.nonzero()[0]\n",
    "    # if np.sum(label >= label_thres) < k: return None\n",
    "    label, score = label[nnz], score[nnz]\n",
    "    label = (label >= label_thres).astype(int)\n",
    "    return utils.ndcg_score(label, score, k)\n",
    "\n",
    "def all_user_ndcg(label, pred_mat, cond_fn, label_thres=4, k=10):\n",
    "    \"\"\"avg of all user ndcg score\"\"\"\n",
    "    tot_ndcg, actual_cnt = 0, 0\n",
    "    for i, (label, score) in enumerate(zip(teRatingMat, pred_mat)):\n",
    "        if not cond_fn(label): continue\n",
    "\n",
    "        ndcg = single_user_ndcg(label, score, k=10)\n",
    "        if ndcg is not None:\n",
    "            tot_ndcg += ndcg\n",
    "            actual_cnt += 1\n",
    "    return tot_ndcg / actual_cnt\n",
    "\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        pred_mat.append(model.predict(sess, u_data, items))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall At K: \n",
    "> **對於每個user喜歡的前10部電影中(rating >= 4), 和預測值比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def recall_at_k(truth, pred_mat, condition_fn, k=10, label_thres=4, pred_thres=0.8):\n",
    "    hits, total = 0, 0\n",
    "    for labels, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(labels): continue\n",
    "\n",
    "        top_percentile = np.percentile(pr, pred_thres * 100)\n",
    "        top_k_ind = labels.argsort()[::-1][:k]\n",
    "        hits += sum(pr[top_k_ind] >= top_percentile)\n",
    "        # hits += recall_score(labels >= label_thres, pr >= np.percentile(pr, pred_thres * 100))\n",
    "        total += k\n",
    "    return hits / total\n",
    "    \n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "def recall_cond_fn(label):\n",
    "    \"\"\"recall條件限制為, user在test data正向評價電影至少10部\"\"\"\n",
    "    return sum(label >= 4) >= 10\n",
    "    \n",
    "print( recall_at_k(teRatingMat, pred_mat, condition_fn=recall_cond_fn) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## MF + DNN with Cross Entropy Loss Function (Sigmoid Cross Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在Rating只有1(正向) or 0(負向)時該如何處理?\n",
    "1. 繼承ModelMfDNN, 改動loss function, 因為使用sigmoid, rating欄位需要變成0 or 1\n",
    "2. 每一個epoch觀察cross entropy的loss之外, 也觀察RMSE loss的變化(計算RMSE維持原rating)\n",
    "3. 許多狀況很難拿到user explicit feedback, 像是movie rating, 因為沒法強迫user去rating所有movies, \n",
    "   但是應該都能夠蒐集到implicit feedback\n",
    "   ex: youtube影片觀看時間長短, 點擊率, 通常implicit feedback只能歸納為到 [0, 1] 之間, 0代表negative, 1代表positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMfDNNCrossEntropy(ModelMfDNN):\n",
    "    def __init__(self,\n",
    "                 n_items,\n",
    "                 n_genres,\n",
    "                 dim=32,\n",
    "                 learning_rate=0.01,\n",
    "                 modelDir=\"./model/model_mf_with_dnn_xent\"):\n",
    "        \"\"\"初始化 ModelBase Tensorflow Graph\"\"\"\n",
    "        self.n_items = n_items\n",
    "        self.n_genres = n_genres\n",
    "        self.ftr_cols = OrderedDict()\n",
    "\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.isTrain = tf.placeholder(tf.bool, None)\n",
    "                # user data\n",
    "                self.user_id = tf.placeholder(tf.int32, [None])\n",
    "                self.query_movie_ids = tf.placeholder(tf.int32, [None, None])\n",
    "                self.query_movie_ids_len = tf.placeholder(tf.int32, [None])\n",
    "                # item data\n",
    "                self.genres = tf.placeholder(tf.int32, [None, None])\n",
    "                self.genres_len = tf.placeholder(tf.int32, [None])\n",
    "                self.avg_rating = tf.placeholder(tf.float32, [None])\n",
    "                self.year = tf.placeholder(tf.float32, [None])\n",
    "                self.candidate_movie_id = tf.placeholder(tf.int32, [None])\n",
    "                # labels\n",
    "                self.rating = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            init_fn = tf.glorot_normal_initializer()\n",
    "            emb_init_fn = tf.glorot_uniform_initializer()\n",
    "            self.b_global = tf.Variable(emb_init_fn(shape=[]), name=\"b_global\")\n",
    "            with tf.variable_scope(\"embedding\"):\n",
    "                self.w_query_movie_ids = tf.Variable(emb_init_fn(shape=[self.n_items, dim]), name=\"w_query_movie_ids\")\n",
    "                self.b_query_movie_ids = tf.Variable(emb_init_fn(shape=[dim]), name=\"b_query_movie_ids\")\n",
    "                self.w_candidate_movie_id = tf.Variable(init_fn(shape=[self.n_items, dim]), name=\"w_candidate_movie_id\")\n",
    "                self.b_candidate_movie_id = tf.Variable(init_fn(shape=[dim + 8 + 2]), name=\"b_candidate_movie_id\")\n",
    "                self.w_genres = tf.Variable(emb_init_fn(shape=[self.n_genres, 8]), name=\"w_genres\")\n",
    "\n",
    "                # query_movie embedding\n",
    "                self.query_emb = tf.nn.embedding_lookup(self.w_query_movie_ids, self.query_movie_ids)\n",
    "                query_movie_mask = tf.expand_dims(tf.nn.l2_normalize(tf.to_float(tf.sequence_mask(self.query_movie_ids_len)), 1), -1)\n",
    "                self.query_emb = tf.reduce_sum(self.query_emb * query_movie_mask, 1)\n",
    "                self.query_bias = tf.matmul(self.query_emb, self.b_query_movie_ids[:, tf.newaxis])\n",
    "\n",
    "                # candidate_movie embedding\n",
    "                self.candidate_emb = tf.nn.embedding_lookup(self.w_candidate_movie_id, self.candidate_movie_id)\n",
    "\n",
    "                # genres embedding\n",
    "                self.genres_emb = tf.nn.embedding_lookup(self.w_genres, tf.to_int32(self.genres))\n",
    "                genres_mask = tf.expand_dims( tf.nn.l2_normalize(tf.to_float(tf.sequence_mask( tf.reshape(self.genres_len, [-1])) ), 1), -1)\n",
    "                self.genres_emb = tf.reduce_sum(self.genres_emb * genres_mask, 1)\n",
    "\n",
    "            # encode [item embedding + item metadata]\n",
    "            with tf.variable_scope(\"dnn\"):\n",
    "                self.item_repr = tf.concat([self.candidate_emb, self.genres_emb, self.avg_rating[:, tf.newaxis], self.year[:, tf.newaxis]], 1)\n",
    "                self.candidate_bias = tf.matmul(self.item_repr, self.b_candidate_movie_id[:, tf.newaxis])\n",
    "                \n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, 0.5, training=self.isTrain)\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, 0.5, training=self.isTrain)\n",
    "                self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "                # self.item_repr = tf.layers.dropout(self.item_repr, 0.5, training=self.isTrain)\n",
    "\n",
    "            with tf.variable_scope(\"computation\"):\n",
    "                infer = tf.reduce_sum(self.query_emb * self.item_repr, 1, keep_dims=True)\n",
    "                infer = tf.add(infer, self.b_global)\n",
    "                infer = tf.add(infer, self.query_bias)\n",
    "                self.infer = tf.add(infer, self.candidate_bias, name=\"infer\")\n",
    "\n",
    "                # one query for all items\n",
    "                self.pred = tf.matmul(self.query_emb, tf.transpose(self.item_repr)) + \\\n",
    "                            tf.reshape(self.candidate_bias, (1, -1)) + self.query_bias + self.b_global\n",
    "                self.pred = tf.nn.sigmoid(self.pred)\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                # if rating >= 4 then 1 else 0\n",
    "                self.alter_rating = tf.to_float(self.rating >= 4)\n",
    "                self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.alter_rating[:, np.newaxis], logits=self.infer))\n",
    "\n",
    "                # for eval\n",
    "                self.rmse_loss = tf.sqrt(tf.losses.mean_squared_error(labels=self.alter_rating[:, tf.newaxis], predictions=self.infer))\n",
    "                self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.alter_rating[:, tf.newaxis]))\n",
    "                pass\n",
    "\n",
    "            with tf.variable_scope(\"train\"):\n",
    "                # try: 選擇不同的optimizer\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)\n",
    "                # self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "                pass\n",
    "\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            self.graph = graph\n",
    "            self.modelDir = modelDir\n",
    "            \n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if reset:\n",
    "            print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "            self.resetModel(self.modelDir)\n",
    "        # try: 試著重上次儲存的model再次training\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"XEntropy Error\", \"Val XENT Error\", \"Val RMSE Error\", \"Elapsed Time\"))\n",
    "        minLoss = 1e7\n",
    "        for ep in range(1, nEpoch + 1):\n",
    "            tr_loss, tr_total =  0, 0\n",
    "            for i, data in enumerate(trainGen(), 1):\n",
    "                loss, _ = sess.run([self.loss, self.train_op], feed_dict=self.feed_dict(data, mode=\"train\"))\n",
    "                batch_len = len(data[\"query_movie_ids\"])\n",
    "                tr_loss += loss * batch_len\n",
    "                tr_total += batch_len\n",
    "                print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "                \n",
    "            if testGen is not None:\n",
    "                te_rmse_loss, te_xent_loss  = self.epochLoss(sess, testGen)\n",
    "\n",
    "            tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "            if minLoss > te_xent_loss:\n",
    "                tpl += \", saving ...\"\n",
    "                self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "                minLoss = te_xent_loss\n",
    "\n",
    "            end = time.time()\n",
    "            print(tpl % (ep, tr_loss / tr_total, te_xent_loss, te_rmse_loss, end - start))\n",
    "            start = end\n",
    "        return self\n",
    "\n",
    "    def epochLoss(self, sess, dataGen):\n",
    "        tot_xent_loss, tot_rmse_loss, tot_cnt = 0, 0, 0\n",
    "        for data in dataGen():\n",
    "            xent_loss, rmse_loss = sess.run([self.loss, self.rmse_loss], feed_dict=self.feed_dict(data, mode=\"eval\"))\n",
    "            batch_len = len(data[\"query_movie_ids\"])\n",
    "            tot_rmse_loss += rmse_loss ** 2 * batch_len\n",
    "            tot_xent_loss += xent_loss * batch_len\n",
    "            tot_cnt += batch_len\n",
    "        return np.sqrt(tot_rmse_loss / tot_cnt), tot_xent_loss / tot_cnt\n",
    "    \n",
    "    \n",
    "learning_rate = 0.1\n",
    "dim = 16\n",
    "modelDir = \"./model/model_mf_with_dnn_xent\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model_xent = ModelMfDNNCrossEntropy(\n",
    "                n_items=nMovies,\n",
    "                n_genres=n_genres,\n",
    "                dim=dim,\n",
    "                learning_rate=learning_rate,\n",
    "                modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_batch = 128\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    model_xent.fit(sess, dataFn(trProcessed, n_batch=n_batch, shuffle=True), dataFn(teProcessed, n_batch=n_batch), nEpoch=10, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_CURVE (Receiver operating characteristic), AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def drawRocCurve(y, predProba):\n",
    "    fprRf, tprRf, _ = roc_curve(y, predProba, pos_label=1)\n",
    "    aucScr = auc(fprRf, tprRf)\n",
    "    print(\"auc:\", aucScr)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fprRf, tprRf, label='ROC CURVE')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('AOC: Area Under Curve (score: {:.4f})'.format(aucScr))\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    predMat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        predMat.append(model_xent.predict(sess, u_data, items))\n",
    "    predMat = np.vstack(predMat)\n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 觀察單一user與預測分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# user id from 0 ~ 670\n",
    "uid = 26\n",
    "u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred = model_xent.predict(sess, u_queries, movies_meta)\n",
    "print(\"shape: \", pred.shape, minmax_scale(pred.T).T)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribution\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribution\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 26\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "    recomm = model_xent.predict(sess, u_queries, movies_meta).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)), \n",
    "              \"title\": midMap[np.arange(len(recomm))].values, \n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision At 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def precision_at_k(truth, pred_mat, condition_fn=None, k=10, label_thres=4):\n",
    "    hits, total = 0, 0\n",
    "    for label, pr in zip(truth, pred_mat):\n",
    "        if not condition_fn(label): continue\n",
    "\n",
    "        top_k_ind = (pr * (label != 0)).argsort()[::-1][:k]\n",
    "        hits += sum(label[top_k_ind] >= label_thres)\n",
    "        total += k\n",
    "    return hits / total\n",
    "\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model_xent.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        pred_mat.append(model_xent.predict(sess, u_data, items))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## 取出movies embedding, 使用cosine similarity列出最相似的電影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies[movies.title.str.contains(\"Toy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def most_like(model, seed_movie, k=10):\n",
    "    \"\"\"給定某一部電影, 使用model裡movies embedding找尋cosine相似度高的其他電影!\"\"\"\n",
    "    with tf.Session(graph=model.graph) as sess:\n",
    "        model.ckpt(sess, model.modelDir)\n",
    "        user_queries, items = list(user_item_data(trProcessed, [0], movie_trans))[0]\n",
    "        movie_emb = sess.run(model.item_repr, feed_dict={\n",
    "            model.isTrain: False,\n",
    "            model.genres: items[\"genres\"],\n",
    "            model.genres_len: items[\"genres_len\"],\n",
    "            model.avg_rating: items[\"avg_rating\"],\n",
    "            model.year: items[\"year\"],\n",
    "            model.candidate_movie_id: items[\"candidate_movie_id\"]\n",
    "        })\n",
    "        \n",
    "    most_like = cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb).ravel().argsort()[::-1][:k]\n",
    "    return movies.iloc[most_like]\n",
    "\n",
    "# mse訓練出來的model\n",
    "most_like(model, 0, k=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cross entropy訓練出來的model\n",
    "most_like(model_xent, 0, k=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Observations\n",
    "1. 可觀察到使用movie embedding以cosine相似度找相似的movie已經有效果, 但是我們用的並不是自己組合出來的movie features, 而是embedding, 代表在training當中已經讓embedding是有意義的代表movie.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
